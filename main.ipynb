{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502b89c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import uuid\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06a7984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"\"\"You are a Llama model, a Large Language Model (LLM) created by Meta.\n",
    "# You power an AI assistant called Tour Planner.\n",
    "# The current date is {today}.\n",
    "\n",
    "# When you're not sure about some information, you say that you don't have the information and don't make up anything.\n",
    "# You are always very attentive to dates, in particular you try to resolve dates (e.g. \"yesterday\" is {yesterday}) and when asked about information at specific dates, you discard information that is at another date.\n",
    "# You follow these instructions in all languages, and always respond to the user in the language they use or request.\n",
    "# Next sections describe the capabilities that you have.\n",
    "\n",
    "# # WEB BROWSING INSTRUCTIONS\n",
    "\n",
    "# You cannot perform any web search or access internet to open URLs, links etc. If it seems like the user is expecting you to do so, you clarify the situation and ask the user to copy paste the text directly in the chat.\"\"\"\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B\", torch_dtype=torch.float16, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974c3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Función de normalización del texto\n",
    "# def normalizar_texto(texto: str) -> str:\n",
    "#     # Convertir a minúsculas\n",
    "#     texto = texto.lower()\n",
    "#     # Eliminar acentos y diacríticos\n",
    "#     texto = ''.join(\n",
    "#         c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn'\n",
    "#     )\n",
    "#     # Eliminar caracteres no alfabéticos (como signos de interrogación)\n",
    "#     texto = re.sub(r'[^a-z0-9\\s]', '', texto)\n",
    "#     return texto\n",
    "\n",
    "# # Simulación de una función de consulta que podría buscar en una base de datos\n",
    "# def consulta_base_datos(query: str):\n",
    "#     # Diccionario de base de datos con claves más sencillas\n",
    "#     data = {\n",
    "#         \"ultimo campeon\": \"El último campeón de la Champions League fue el Real Madrid.\",\n",
    "#         \"mejor equipo\": \"El mejor equipo actualmente es el Manchester City.\"\n",
    "#     }\n",
    "\n",
    "#     # Normalizar la consulta para hacer la comparación más flexible\n",
    "#     query_normalizada = normalizar_texto(query)\n",
    "    \n",
    "#     # Buscar en la base de datos\n",
    "#     for clave in data:\n",
    "#         if normalizar_texto(clave) in query_normalizada:\n",
    "#             return data[clave]\n",
    "    \n",
    "#     return \"No encontré información sobre eso.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22e1bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consulta_tool = Tool(\n",
    "#     name=\"ConsultaDB\",\n",
    "#     func=consulta_base_datos,\n",
    "#     description=\"Consulta información deportiva en la base de datos.\"\n",
    "# )\n",
    "\n",
    "# # Agente de consulta para interactuar con la base de datos\n",
    "# def agente_consulta(state):\n",
    "#     user_input = state.input  # Acceso usando el atributo 'input'\n",
    "    \n",
    "#     # Recuperamos la información relevante de la base de datos\n",
    "#     db_response = consulta_tool.run(user_input)\n",
    "    \n",
    "#     # Si encontramos información, utilizamos el LLM para generar una respuesta más completa\n",
    "#     if db_response != \"No encontré información sobre eso.\":\n",
    "#         query_with_context = f\"Pregunta: {user_input}. Respuesta de la base de datos: {db_response}. ¿Puedes generar una respuesta más detallada?\"\n",
    "#         result = pipe(system_prompt + \"\\n\" + query_with_context)  # El LLM genera una respuesta enriquecida con la base de datos\n",
    "#     else:\n",
    "#         result = db_response  # Si no encontramos información en la base de datos, devolvemos el mensaje por defecto\n",
    "    \n",
    "#     return {\"response\": result, \"next_node\": None}\n",
    "\n",
    "# # Agente Controlador que decide cuándo usar el agente de consulta\n",
    "# def agente_controlador(state):\n",
    "#     user_input = state.input  # Acceso usando el atributo 'input'\n",
    "#     if \"campeón\" in user_input or \"mejor equipo\" in user_input:\n",
    "#         return {\"next_node\": \"consulta\"}  # Enviamos al nodo 'consulta'\n",
    "#     else:\n",
    "#         return {\"response\": \"No tengo información relevante sobre eso.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee6936ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definir un esquema de estado usando Pydantic\n",
    "# class StateSchema(BaseModel):\n",
    "#     input: str\n",
    "#     response: str\n",
    "#     next_node: Optional[str] = None \n",
    "\n",
    "# # Definir el grafo con LangGraph\n",
    "# graph = StateGraph(state_schema=StateSchema)\n",
    "# graph.add_node(\"controlador\", agente_controlador)\n",
    "# graph.add_node(\"consulta\", agente_consulta)  # Añadimos el nodo de consulta\n",
    "# graph.set_entry_point(\"controlador\")\n",
    "# graph.add_edge(\"controlador\", \"consulta\")  # Relacionamos los nodos\n",
    "# graph.add_edge(\"consulta\", END)  # Finalizamos el flujo después de la consulta\n",
    "\n",
    "# dialogue_manager = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8526e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ejecución de prueba\n",
    "# state = StateSchema(input=\"¿Quién es el último campeón?\", response=\"\", next_node=None)\n",
    "# output = dialogue_manager.invoke(state)\n",
    "# print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
