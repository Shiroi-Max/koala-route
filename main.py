"""
Script principal para interactuar con el sistema RAG + LLM de forma continua.

- Recupera contexto desde Azure Cognitive Search usando embeddings.
- Genera respuestas con el modelo LLM configurado (ej. Gemma, Mistral, etc.).
- Acepta mÃºltiples preguntas consecutivas desde la consola.
- Finaliza escribiendo 'salir' o presionando Ctrl+C.

Requiere:
- `controller_agent` desde agents.py

Uso:
    python main.py
"""

from modules.agents import controller_agent


def main():
    """
    Ejecuta una sesiÃ³n interactiva para preguntas al sistema RAG.
    """
    print("ğŸ§  Sistema RAG + LLM (Escribe 'salir' para terminar)\n")

    try:
        while True:
            user_query = input("â“ Tu pregunta: ").strip()

            if user_query.lower() in {"salir", "exit", "quit"}:
                print("ğŸ‘‹ Cerrando sesiÃ³n.")
                break

            if not user_query:
                continue  # Ignorar entrada vacÃ­a

            respuesta = controller_agent(user_query=user_query)

            print("\nğŸ’¬ Respuesta:\n", respuesta, "\n")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ SesiÃ³n interrumpida manualmente.")


if __name__ == "__main__":
    main()
