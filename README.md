# ğŸ§  RAG + LLM con LangChain, Hugging Face y Azure AI Search

Este proyecto implementa un sistema de RecuperaciÃ³n y GeneraciÃ³n aumentada (RAG) que:

- Usa Azure Cognitive Search como base vectorial.
- Aplica embeddings locales con `sentence-transformers`.
- Utiliza un modelo LLM local (ej. Gemma 2B, Mistral 7B) para generar respuestas.
- Funciona con PyTorch y aceleraciÃ³n GPU (CUDA 12.1).

---

## ğŸš€ Requisitos

- Python **3.12**
- GPU compatible con **CUDA 12.1**
- Dependencias gestionadas con `pyproject.toml`

---

## âš¡ InstalaciÃ³n

### 1. Crea y activa un entorno virtual

En Linux/macOS:

    python3 -m venv .venv
    source .venv/bin/activate

En Windows:

    python -m venv .venv
    .venv\Scripts\activate

---

### 2. Instala PyTorch con soporte GPU (CUDA 12.1)

> ğŸ› ï¸ Este paso es **obligatorio** antes de instalar el resto.

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 3. Instala el resto del proyecto

```bash
pip install .
```

Esto instalarÃ¡ LangChain, Azure, Hugging Face, sentence-transformers, widgets y mÃ¡s.

---

## ğŸ§ª Uso

### Iniciar una sesiÃ³n interactiva

```bash
python main.py
```

Puedes hacer preguntas consecutivas al modelo.  
Escribe `salir` para cerrar la sesiÃ³n.

---

### Subir documentos al Ã­ndice

```bash
python uploader.py --file info.txt --title "LangChain Overview"
```

El archivo debe estar en la carpeta `data/`.

---

### Eliminar documentos por ID
```bash
    python deleter.py --id <id_documento>
```
---

## ğŸ“‚ Estructura del proyecto

    rag_llm_project/
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ config.py
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ info.txt
    â”œâ”€â”€ modules/
    â”‚   â”œâ”€â”€ agents.py
    â”‚   â”œâ”€â”€ embeddings.py
    â”‚   â”œâ”€â”€ llm.py
    â”œâ”€â”€ main.py
    â”œâ”€â”€ uploader.py
    â”œâ”€â”€ deleter.py
    â”œâ”€â”€ pyproject.toml
    â””â”€â”€ README.md

---

## âœ… Estado

- âœ… Entrenamiento y generaciÃ³n local funcionando
- âœ… Compatible con LangChain 0.2.x y `langchain-huggingface`
- âœ… IndexaciÃ³n y bÃºsqueda en Azure Cognitive Search
- âœ… Soporte para modelos ligeros (DistilBERT, Flan-T5) o potentes (Gemma, Mistral)

---

## ğŸ“Œ Notas

- Puedes adaptar fÃ¡cilmente para servirlo con FastAPI, Streamlit o Gradio.
- Admite cualquier modelo Hugging Face compatible con `transformers.pipeline`.