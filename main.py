"""
Script principal para interactuar con el sistema RAG + LLM de forma continua usando POO y LangGraph.

- Recupera contexto desde Azure Cognitive Search usando embeddings (RetrieverAgent).
- Genera respuestas con el modelo LLM configurado (LLMAgent, ej. Mistral, Gemma...).
- Orquesta el flujo completo mediante ControllerAgent y LangGraph.
- Acepta m√∫ltiples preguntas consecutivas desde la consola.
- Finaliza escribiendo 'salir' o presionando Ctrl+C.

Requiere:
- modules.graph.graph.build_langgraph_controller_flow

Uso:
    python main.py
"""

from modules.graph.agent_state import AgentState
from modules.graph.graph import build_langgraph_controller_flow


def main():
    """
    Ejecuta una sesi√≥n interactiva usando el grafo LangGraph.
    """
    print("üß† Sistema RAG + LLM (Escribe 'salir' para terminar)\n")

    # Construir grafo
    dialogue_manager = build_langgraph_controller_flow()

    try:
        while True:
            user_query = input("‚ùì Tu pregunta: ").strip()

            if user_query.lower() in {"salir", "exit", "quit"}:
                print("üëã Cerrando sesi√≥n.")
                break

            if not user_query:
                continue  # Ignorar entrada vac√≠a

            # Ejecutar flujo LangGraph con estado inicial
            state = AgentState(input=user_query, response="")
            result = dialogue_manager.invoke(state)

            print("\nüí¨ Respuesta:\n", result.get("response"), "\n")

    except KeyboardInterrupt:
        print("\nüëã Sesi√≥n interrumpida manualmente.")
    except Exception as e:
        print(f"\n‚ö†Ô∏è Ocurri√≥ un error inesperado: {e}")


if __name__ == "__main__":
    main()
